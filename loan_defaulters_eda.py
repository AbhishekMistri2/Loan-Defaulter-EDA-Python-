# -*- coding: utf-8 -*-
"""Loan Defaulters EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15uPNuyK_C1erzV1PEn7dyXuxMpTNTDla

# **About Project**

## **About dataset**
"""

# This case study aims to give us an idea of applying EDA in a real business scenario.
# In this case study, apart from applying the techniques that we have learnt in the EDA module,
# we will also develop a basic understanding of risk analytics in banking and financial services and
# understand how data is used to minimise the risk of losing money while lending to customers.

"""## **Objectives**"""

# 1.	Feature Selection
# 2.	Feature Engineering
# 3.	Missing value imputation
# 4.	Binning
# 5.	Variable Derivation
# 6.	Variables segmentation
# 7.	Data Exploration
# 8.	Behaviour observation
# 9.	Insight Derivation
# 10.	Visualization

"""# **Import Pakages**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Import Datasets**"""

app = pd.read_csv("/content/application_data.csv")
prev_app = pd.read_csv("/content/previous_application.csv")

"""# **Feature Selection**

## Data Exploration
"""

# Get first 10 row
app.head(10)

# Get Columns names
app.columns

# Get shape of data
app.shape

# Get null value
app.isnull().sum().sort_values()

# Make diffrent missing info table
missing_info = pd.DataFrame(app.isnull().sum().sort_values().reset_index())

# Rename missing info
missing_info.rename(columns = {"index": "col_name",0:"null_count"},inplace = True)

#Print missing info
missing_info

# find null count
missing_info["missing_percentage"]= missing_info["null_count"]/app.shape[0]*100

# Print
missing_info

# Export files
missing_info.to_csv("Missing_info_Data.csv", index = False)

# Arrange missing value

missing_col = missing_info[missing_info["missing_percentage"]>= 40]["col_name"].to_list()
missing_col

# Removed missing value dataset

app_rmv = app.drop(labels = missing_col, axis =1 )

app_rmv.shape

# Flag

flag_col = []

for col in app_rmv.columns:
  if col.startswith("FLAG_"):
    flag_col.append(col)

flag_col

# Length of flag col
len(flag_col)

# flag columns and target

app_rmv[flag_col+["TARGET"]]

# flag column and target data
flagtar_col = app_rmv[flag_col+["TARGET"]]

# visualize
plt.figure(figsize = (25,30))

for i, col in enumerate(flag_col):
  plt.subplot(7,4,i+1)
  sns.countplot(data = flagtar_col, x = col, hue = "TARGET")

# specific flag columns
flag_corr = ['FLAG_OWN_CAR','FLAG_OWN_REALTY','FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL','TARGET']

# remaning columns
flag_corr_df = app_rmv[flag_corr]

# Get correnation
corr_df = round(flag_corr_df.corr(),2)

#print
corr_df

# heatmap

sns.heatmap(corr_df, cmap= 'rocket_r', linewidth = 1, annot = True)

# flag car group by count

flag_corr_df.groupby(["FLAG_OWN_CAR"]).size()

# replace value

flag_corr_df["FLAG_OWN_CAR"] = flag_corr_df["FLAG_OWN_CAR"].replace({'N': 0,'Y':1})
flag_corr_df["FLAG_OWN_REALTY"] = flag_corr_df["FLAG_OWN_REALTY"].replace({'N': 0,'Y':1})

flag_corr_df.groupby(["FLAG_OWN_CAR"]).size()

# remove flag columns

app_flag_r = app_rmv.drop(labels = flag_col,axis = 1)

# shape of app_flag_r
app_flag_r.shape

# top 5 row of app_flag_r
app_flag_r.head()

# Removing scores
app_scr = app_flag_r.drop(["EXT_SOURCE_2","EXT_SOURCE_3"],axis = 1)

# shape of App_scr
app_scr.shape

"""# **Feature Enginearring**"""

# Find null value
app_scr.isnull().sum().sort_values()

"""# **Missing Value Imputation**"""

# group by

app_scr.groupby("CNT_FAM_MEMBERS").size()

# find mode

app_scr["CNT_FAM_MEMBERS"] = app_scr["CNT_FAM_MEMBERS"].fillna((app_scr["CNT_FAM_MEMBERS"].mode()[0]))

# find null value

app_scr["CNT_FAM_MEMBERS"].isnull().sum()

# group by occupation

app_scr.groupby("OCCUPATION_TYPE").size().sort_values()

# fill null

app_scr["OCCUPATION_TYPE"] = app_scr["OCCUPATION_TYPE"].fillna((app_scr["OCCUPATION_TYPE"].mode()[0]))

# null value

app_scr["OCCUPATION_TYPE"].isnull().sum()

# group by name type

app_scr.groupby("NAME_TYPE_SUITE").size()

# find mode of name_type_suit

app_scr["NAME_TYPE_SUITE"].mode()

# fill null value by mode value

app_scr["NAME_TYPE_SUITE"] = app_scr["NAME_TYPE_SUITE"].fillna((app_scr["NAME_TYPE_SUITE"].mode()[0]))

# find null vaule count

app_scr["NAME_TYPE_SUITE"].isnull().sum()

"""## Missing value"""

# description of amt_annuity

app_scr["AMT_ANNUITY"].describe()

# fill null value with mode

app_scr["AMT_ANNUITY"] = app_scr["AMT_ANNUITY"].fillna((app_scr["AMT_ANNUITY"].mode()))

# find null value

app_scr["AMT_ANNUITY"].isnull().sum()

# find all amt_req_credit value at once

amount_req_col = []

for col in app_scr.columns:
  if col.startswith("AMT_REQ_CREDIT_BUREAU"):
    amount_req_col.append(col)


amount_req_col

# fill null value with median for all amt_req_credit at once

for col in amount_req_col:
  app_scr[col] = app_scr[col].fillna((app_scr[col].median()))

# find null value

app_scr[col].isnull().sum()

# find null value

app_scr["AMT_GOODS_PRICE"].isnull().sum()

# Aggregates of amt_goods_price

app_scr["AMT_GOODS_PRICE"].agg(["min","max", "median"])

# fill null vaule by mode

app_scr["AMT_GOODS_PRICE"] = app_scr["AMT_GOODS_PRICE"].fillna((app_scr["AMT_GOODS_PRICE"].mode()[0]))

# find null value

app_scr["AMT_GOODS_PRICE"].isnull().sum()

# group all days columns in days_col

days_col = []

for col in app_scr.columns:
  if col.startswith("DAYS"):
    days_col.append(col)

days_col

# convert negative num into positive

for col in days_col:
  app_scr[col] = abs(app_scr[col])

# find length of app_scr

len(app_scr.columns)

"""## Outlier Detection & Treatment"""

# description of amt_goods_price

app_scr["AMT_GOODS_PRICE"].describe()

# find aggregate of amt_goods_price

app_scr["AMT_GOODS_PRICE"].agg(["min","max","median"])

# visualize kdeplot of amt_goods_price

sns.kdeplot(data= app_scr, x= "AMT_GOODS_PRICE")

# visualize boxplot of amt_goods_price

sns.boxenplot(data= app_scr, x= "AMT_GOODS_PRICE")

"""# **Binning**"""

# find quartile of amt_goods_price

app_scr["AMT_GOODS_PRICE"].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])

# find max value amt_goods_price

app_scr["AMT_GOODS_PRICE"].max()

# grouping values

bins = [0,100000,200000,300000,400000,500000,600000,700000,800000,900000,2961000]
ranges = ["0-100k","100k-200k","200k-300k","300k-400k","400k-500k","500k-600k","600k-700k","700k-800k","800k-900k","900k-Above"]

app_scr["AMT_GOODS_PRICE_RANGE"] = pd.cut(app_scr["AMT_GOODS_PRICE"], bins, labels = ranges)

# print amt_goods_price range

app_scr.groupby(["AMT_GOODS_PRICE_RANGE"]).size()

# find quartile of amt_income_total

app_scr["AMT_INCOME_TOTAL"].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])

# find max value of amt_income_total

app_scr["AMT_INCOME_TOTAL"].max()

# grouping value of amt_income_total

bins = [0,100000,150000,200000,250000,300000,350000,400000,1935000,]
ranges = ["0-100k","100k-150k","150k-200k","200k-250k","250k-300k","300k-350k","350k-400k","400k-Above"]

app_scr["AMT_INCOME_TOTAL_RANGE"] = pd.cut(app_scr["AMT_INCOME_TOTAL"], bins, labels = ranges)

# print value of amt_income_total_range

app_scr.groupby(["AMT_INCOME_TOTAL_RANGE"]).size()

# find quartile of amt_credit

app_scr["AMT_CREDIT"].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])

# # find max value of amt_credit

app_scr["AMT_CREDIT"].max()

# group value of amt_credit

bins = [0,300000,600000,900000,1200000,1500000,1800000,2961000]
ranges = ["0-300k","300k-600k","600k-900k","900k-1200k","1200k-1500k","1500k-1800k","1800k-Above"]

app_scr["AMT_CREDIT_RANGE"] = pd.cut(app_scr["AMT_CREDIT"], bins, labels = ranges)

# print  value of amt_credit_range

app_scr.groupby(["AMT_CREDIT_RANGE"]).size()

# find quartile of amt_annuity

app_scr["AMT_ANNUITY"].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])

# find max value of amt_annuity

app_scr["AMT_ANNUITY"].max()

# group value of amt_annuity

bins = [0,10000,20000,30000,40000,50000,60000,70000,135936,]
ranges = ["0-100k","100k-200k","200k-300k","300k-400k","400k-500k","500k-600k","600k-700k","700k-Above"]

app_scr["AMT_ANNUITY_RANGE"] = pd.cut(app_scr["AMT_ANNUITY"], bins, labels = ranges)

# print value of amt_annuity_range

app_scr.groupby(["AMT_ANNUITY_RANGE"]).size()

# find quartile of days_employed

app_scr["DAYS_EMPLOYED"].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])

# find max value of days_employed

app_scr["DAYS_EMPLOYED"].max()

# group value of days_employed

bins = [0,1825, 3650, 5475, 7300, 9125, 10950, 12775, 14600, 16425, 18250, 365243]
ranges = ['0-5Y', '5Y-10Y', '10Y-15Y', '15Y-20Y', '20Y-25Y', '25Y-30Y', '30Y-35Y', '35Y-40Y', '40Y-45Y', '45Y-50Y', 'Above 50Y']

app_scr["DAYS_EMPLOYED_RANGE"] = pd.cut(app_scr["DAYS_EMPLOYED"], bins, labels = ranges)

# print value of days_employed_range

app_scr.groupby(["DAYS_EMPLOYED_RANGE"]).size()

"""# **Data Analysis | Visualization**"""

# info about app_scr

app_scr.info()

# find app_scr datatypes

app_scr.dtypes.value_counts()

# find app_scr object datatype

app_scr.select_dtypes(include=["object"]).head(5)

obj_var = app_scr.select_dtypes(include=["object"]).columns

# Name contract type size

app_scr.groupby(["NAME_CONTRACT_TYPE"]).size()

"""**Univarient** **categorical**"""

# countplot

sns.countplot(data= app_scr, x = "NAME_CONTRACT_TYPE", hue = "TARGET")

# find name contract type mean value

app_scr[["NAME_CONTRACT_TYPE","TARGET"]].groupby(["NAME_CONTRACT_TYPE"],as_index = False).mean()

# give name as data_pct

data_pct = app_scr[["NAME_CONTRACT_TYPE","TARGET"]].groupby(["NAME_CONTRACT_TYPE"],as_index = False).mean()

# genrate percentage column

data_pct["PCT"]= data_pct["TARGET"]*100

# print data_pct

data_pct

# barplot

sns.barplot(data= data_pct, x = "NAME_CONTRACT_TYPE", y="PCT")

# combined graph

plt.figure(figsize = (10,5))

plt.subplot(1,2,1)
sns.countplot(data= app_scr, x = "NAME_CONTRACT_TYPE", hue = "TARGET")

plt.subplot(1,2,2)
sns.barplot(data= data_pct, x = "NAME_CONTRACT_TYPE", y="PCT")

# combined graph of data_pct

plt.figure(figsize = (15,70))

for i, var in enumerate (obj_var):

  data_pct = app_scr[[var,"TARGET"]].groupby([var],as_index = False).mean()
  data_pct["PCT"]= data_pct["TARGET"]*100

  plt.subplot(10,2,i+i+1)
  plt.subplots_adjust(wspace =0.1, hspace = 1)
  sns.countplot(data= app_scr, x = var, hue = "TARGET")
  plt.xticks(rotation = 90)

  plt.subplot(10,2,i+i+2)
  plt.subplots_adjust(wspace =0.1, hspace = 1)
  sns.barplot(data= data_pct, x = var, y="PCT", palette = "rocket_r")
  plt.xticks(rotation = 90)

# find flot64 and int64 columns

num_var = app_scr.select_dtypes(include=["float64","int64"]).columns

# find flot64, int64  and category columns

num_catvar = app_scr.select_dtypes(include=["float64","int64","category"]).columns

# print num_var

num_var

# print top 5 row value of app_scr

app_scr[num_var].head()

# name table as num_data

num_data = app_scr[num_var]

# print top 5 row

num_data.head()

# group by
num_data.groupby(["TARGET"]).size()/num_data.shape[0]*100

# name table as defaulters

defaulters = num_data[num_data["TARGET"]==1].drop(["TARGET"],axis = 1)

# print

defaulters

# name table as Repayers

repayers = num_data[num_data["TARGET"]==0].drop(["TARGET"],axis = 1)

# print

repayers

# name table as def_cor

def_cor = defaulters.corr()

# print

def_cor

# privide name for table by remaning column heading

def_corru = def_cor.where(np.triu(np.ones(def_cor.shape),k=1).astype(bool)).unstack().reset_index().rename(columns={"level_0":"var1","level_1": "var2", 0:"corr"})

# genrate absolute value

def_corru["corr"] = abs(def_corru["corr"])

# drop null value corr and arrenge in asending order

def_corru.dropna(subset=["corr"]).sort_values(by=["corr"], ascending = False)

# name table as rep_cor after genrating repayers correlation

rep_cor = repayers.corr()

# print

rep_cor

# privide name for table by remaning column heading

rep_corru = rep_cor.where(np.triu(np.ones(rep_cor.shape),k=1).astype(bool)).unstack().reset_index().rename(columns={"level_0":"var1","level_1": "var2", 0:"corr"})

# print

rep_corru

# provide absolute value for corr

rep_corru["corr"] = abs(rep_corru["corr"])

# drop corr and sort data in asending order

rep_corru.dropna(subset=["corr"]).sort_values(by=["corr"], ascending = False)

"""**Univarient Numerical**"""

# top 5 row

num_data.head()

# name table amt_var by providing values

amt_var = ["AMT_INCOME_TOTAL","AMT_CREDIT","AMT_ANNUITY","AMT_GOODS_PRICE"]

# visualize kdeplot

sns.kdeplot(data= num_data, x= "AMT_CREDIT", hue = "TARGET")

# genrate multiple kdeplot at once using for loop

plt.figure(figsize=(10,5))

for i, col in enumerate(amt_var):
  plt.subplot(2,2,i+1)
  sns.kdeplot(data=num_data, x=col, hue="TARGET")
  plt.subplots_adjust(wspace= 0.5, hspace=0.5)

"""**Bivarient Numerical**"""

# visualize scatterplot

sns.scatterplot(data= num_data, x = "AMT_CREDIT", y = "AMT_GOODS_PRICE", hue = "TARGET")

# visulaize scatterplot

sns.scatterplot(data= num_data, x = "AMT_CREDIT", y = "AMT_INCOME_TOTAL", hue = "TARGET")

# visulaize scatterplot

sns.scatterplot(data= num_data, x = "AMT_CREDIT", y = "CNT_CHILDREN", hue = "TARGET")

# name table amt_var

amt_var = num_data[["AMT_INCOME_TOTAL",	"AMT_CREDIT",	"AMT_ANNUITY",	"AMT_GOODS_PRICE", "TARGET"]]

# visulaize pairplot

sns.pairplot(data= amt_var, hue= "TARGET")

"""Prev data"""

# find null value and renaming columns

null_count = pd.DataFrame(prev_app.isnull().sum().sort_values(ascending=False)/prev_app.shape [0]*100).reset_index().rename(columns={'index':'var',0:'count_pct'})

# print
null_count

# find value greater than 40 count

var_mg4 = list(null_count[null_count["count_pct"]>= 40]["var"])

# print

var_mg4

# privide table with value

nva_cols = var_mg4+['WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START', 'FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY']

# print

nva_cols

# drop nva_cols

prev_app_ncr = prev_app.drop(nva_cols,axis=1)


# print length
len(prev_app_ncr.columns)

# top 5 rows

prev_app_ncr.head()

# find null value and sort data

prev_app_ncr.isnull().sum().sort_values(ascending=False)/prev_app_ncr.shape[0]* 100

# print aggregate value

prev_app_ncr['AMT_GOODS_PRICE'].agg(func=['mean','median'])

# fill null value by median

prev_app_ncr['AMT_GOODS_PRICE_MEDIAN'] = prev_app_ncr['AMT_GOODS_PRICE'].fillna(prev_app_ncr['AMT_GOODS_PRICE'].median())

# fill null value by mean

prev_app_ncr['AMT_GOODS_PRICE_MEAN'] = prev_app_ncr['AMT_GOODS_PRICE'].fillna(prev_app_ncr['AMT_GOODS_PRICE'].mean())

# fill null value by mode

prev_app_ncr['AMT_GOODS_PRICE_MODE'] = prev_app_ncr['AMT_GOODS_PRICE'].fillna(prev_app_ncr['AMT_GOODS_PRICE'].mode()[0])

# create table by prividing value

gp_cols = ['AMT_GOODS_PRICE', 'AMT_GOODS_PRICE_MEDIAN', 'AMT_GOODS_PRICE_MEAN','AMT_GOODS_PRICE_MODE']

# visualize

plt.figure(figsize=(10,5))

for i, col in enumerate(gp_cols):
  plt.subplot(2,2,i+1)
  sns.kdeplot(data= prev_app_ncr, x=col)
  plt.subplots_adjust(wspace=0.5,hspace=0.5)

# fill null value by median

prev_app_ncr['AMT_GOODS PRICE'] = prev_app_ncr['AMT_GOODS_PRICE'].fillna (prev_app_ncr['AMT_GOODS_PRICE'].median())

# genrate aggregate value

prev_app_ncr['AMT_ANNUITY'].agg(func=['mean', 'median', 'max'])

# fill null value by median

prev_app_ncr ['AMT_ANNUITY']= prev_app_ncr ['AMT_ANNUITY'].fillna(prev_app_ncr ['AMT_ANNUITY'].median())

# fill null value by mode

prev_app_ncr['PRODUCT_COMBINATION'] = prev_app_ncr['PRODUCT_COMBINATION'].fillna (prev_app_ncr ['PRODUCT_COMBINATION'].mode()[0])

# find null value and sort data

prev_app_ncr[prev_app_ncr['CNT_PAYMENT'].isnull()].groupby(['NAME_CONTRACT_STATUS']).size().sort_values(ascending=False)

# fill null value

prev_app_ncr['CNT_PAYMENT']= prev_app_ncr['CNT_PAYMENT'].fillna(0)

# find null value and sort data

prev_app_ncr.isnull().sum().sort_values(ascending=False)

# print length

len(prev_app_ncr.columns)

"""**Merge Data**"""

# merage tables

merge_df = pd.merge(app_rmv,prev_app_ncr, how ="inner",on = "SK_ID_CURR")

# top 5 rows
merge_df.head()

# visualize counterplot

sns.countplot(data= merge_df, x="NAME_CASH_LOAN_PURPOSE",hue = "NAME_CONTRACT_STATUS")
plt.xticks(rotation= 90)
plt.yscale("log")

# visualize counterplot

sns.countplot(data= merge_df, x="NAME_CONTRACT_STATUS",hue="TARGET")

# merge tables

merge_df.groupby(["NAME_CONTRACT_STATUS","TARGET"]).size()

# groupby data adn rename column

merge_agg = merge_df.groupby(["NAME_CONTRACT_STATUS","TARGET"]).size().reset_index().rename(columns={0:"counts"})

# top 5 rows

merge_agg.head()

# sum of data

sum_df = merge_agg.groupby(["NAME_CONTRACT_STATUS"])["counts"].sum().reset_index()

# merge data from left

merge_agg2=pd.merge(merge_agg,sum_df, how="left", on ="NAME_CONTRACT_STATUS")

# round the value

merge_agg2["pct"] = round(merge_agg2["counts_x"]/merge_agg2["counts_y"]*100,2)

# print

merge_agg2

# visualize lineplot

sns.lineplot(data= merge_df,x="NAME_CONTRACT_STATUS", y="AMT_INCOME_TOTAL",ci=None,hue="TARGET")

"""# **Conclusion | Findings**

### **Findings**
"""

# Data Analysis

# 1)	Object Datatype

    # 1.	NAME_CONTRACT_TYPE- Most of the customers have taken cash loan customers who have taken cash loans are less likely to default

    # 2.	CODE_GENDER – Most of the loans have been taken by female default rate for females are just ~7% which is safer and lesser than male

    # 3.	NAME_TYPE_SUITE - Unaccompanied people had taken most of the loans and the default rate is ~8.5% which is still okay

    # 4.	NAME_INCOME TYPE – The safest segments are working, commercial associates and pensioners

    # 5.	NAME_EDUCATION_TYPE- Higher education is the safest segment to give the loan with a default rate of less than 5%

    # 6.	NAME_FAMILY_STATUS- Married people are safe to target, default rate is 8%

    # 7.	NAME_HOUSING_TYPE- People having house apartment are safe to give loan with default rate at 8%

    # 8.	OCCUPATION_TYPE- Low skilled labours and driver are highest defaulter and accountancy are less defaulter core staff, mangers and Laboral are safe to target with default rate of less than 8% -10%

    # 9.	WEEK_APPAR_PROCESS_START- Transport type 3 is highest defaulter, others, business entity type 3, self-employed are good to go with default rate of 10%

# 2)	Float64, int64

      # Univariate Numeric Variables Analysis

      # 1.	Most of the loans were given for the goods price ranging between 0 to 1 ml
      # 2.	Most of the loans were given for the credit amount of 0 to 1 ml
      # 3.	Most of the customers are paying annuity of 0 to 50k
      # 4.	Mostly the customers have income between 0 to 1 ml

      # Bivariate Analysis

      # 1.	AMT_CREDIT and AMT_GOODS PRICE are linearly corelated, if the AMT_CREDIT increases the defaulters are decreasing
      # 2.	People having income less than or equals to 1 ml, are more like to take loans out of which who are taking loan of less than 1.5 million, could turn out to be default we can target income below 1 million loan amounts greater than 1.5 million
      # 3.	People having children 1to 5 are safer to give loan
      # 4.	People who can pay annuity of 100k are more likely to get loan that upto less than 2 mill

      # Analysis on Merge data

      # 1.	For repairing purpose customer applied previously and it also has the greatest number of cancelations.
      # 2.	Most of applicant who were cancelled or refuse previously paid their loan in time
      # 3.	Previous offers which where unused have max number of default despite having high salary

"""### **Recomendation**"""

# Recommendation

  # 1)	Bank should target the customers

      # 1.	Having low income i.e. Below 1 ml
      # 2.	Working in others, business entity types 3, self-employed org. Type
      # 3.	Working as accountants, core staff, managers and laborers
      # 4.	Having house/apartment and are married and having children not more than 5
      # 5.	Highly educated
      # 6.	Preferably female
      # 7.	Unaccompanied people can be safer - default rate is ~8.5%

  # 2)	Amount segment recommended

      # 1.	The credit amount should not be more than 1 ml
      # 2.	Annuity can be made of 50k (depending on the eligibility)
      # 3.	Income bracket could be below 1 ml
      # 4.	80-90% of the customer who were prev. Cancelled/refused, are repairers. Bank can do the analysis and can consider to give loan to these segments

"""### **Precautions**"""

# Precautions

      # 1.	Org. Transport type 3 should be avoided
      # 2.	Low-skill labourers and drivers should be avoided
      # 3.	Offers previously unused and high-income customer should be avoided